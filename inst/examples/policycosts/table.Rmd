``` {r libraries, include=FALSE}
rm(list=ls())
require(pdgControl)
require(reshape2)
require(ggplot2)
require(data.table)
```

```{r cache-options, include=FALSE}
opts_chunk$set(cache=TRUE, cache.path="table/")
```

```{r graphing-options, include=FALSE}
opts_knit$set(upload.fun = socialR::flickr.url)
opts_chunk$set(dev.args=list(bg="transparent"),
               tidy=FALSE, comment=NA, message=FALSE)
```




``` {r}
fees_fn <- function(sigma_g){ 

price = 10
c0 = 30
profit <- profit_harvest(price = price, c0 = c0, c1 = 0)
c2 <- exp(seq(0, log(21), length.out = 20))-1  # range of c2 values to try
seed <- 123                 # Random seed (replicable results)
delta <- 0.05               # economic discounting rate
OptTime <- 20               # stopping time
gridsize <- 50              # grid size for fish stock and harvest rate (discretized population)
reward <- 0                 # bonus for satisfying the boundary condition
#sigma_g <- 0.2              # Noise in population growth
z_g <- function() rlnorm(1,  0, sigma_g) # mean 1
z_m <- function() 1         # No measurement noise, 
z_i <- function() 1         # No implemenation noise
f <- BevHolt                # Select the state equation
pars <- c(1.5, 0.05)        # parameters for the state equation
K <- (pars[1] - 1)/pars[2]  # Carrying capacity (for reference 
xT <- 0                     # boundary conditions
x0 <- K
x_grid <- seq(0.01, 1.2 * K, length = gridsize)  
h_grid <- x_grid # seq(0.01, 0.8 * K, length = gridsize)  


SDP_Mat <- determine_SDP_matrix(f, pars, x_grid, h_grid, sigma_g )
opt <- find_dp_optim(SDP_Mat, x_grid, h_grid, OptTime, xT, 
                     profit, delta, reward=reward)


L1 <- function(c2) function(h, h_prev)  c2 * abs(h - h_prev) 
free_increase <- function(c2) function(h, h_prev)  c2 * abs(min(h - h_prev, 0)) # increasing harvest is free
free_decrease <- function(c2) function(h, h_prev)  c2 * max(h - h_prev, 0) # decreasing harvest is free
fixed <-  function(c2) function(h, h_prev) c2 * as.numeric( !(h == h_prev) )
L2 <- function(c2) function(h, h_prev)  c2 * (h - h_prev) ^ 2
none <- function(h, h_prev)  0
penaltyfns <- list(L2=L2, L1=L1, free_decrease=free_decrease, fixed=fixed, free_increase=free_increase)


require(snowfall)
sfInit(cpu=8, parallel=T)
sfLibrary(pdgControl)
sfExportAll()


policies <- lapply(penaltyfns, function(penalty){
  sfLapply(c2, function(c2){
      policy <- optim_policy(SDP_Mat, x_grid, h_grid, OptTime, xT, 
                   profit, delta, reward, penalty = penalty(c2))
      }
  )
})


#Note that `optim_policy` has been updated to return the equilibrium value of profits from fish harvests before the adjustment costs have been paid, `penalty_free_V`.  This containst the values for all possible states, we simply evaluate it at the carrying capacity (which is our initial condition.)  The index in `x_grid` that corresponds to the carrying capacity (initial condition) `i` indicates this.  


#Quadratic costs on fishing effort have to be done separately,

quad <- 
  sfLapply(c2, function(c2){
  effort_penalty = function(x,h) (c2 * h / x) / price
  policycost <- optim_policy(SDP_Mat, x_grid, h_grid, OptTime, xT, 
                        profit, delta, reward, penalty = none, 
                        effort_penalty)
})
policies <- c(policies, quad=list(quad))


#`policies` is a list with an entry for each model.  For each model we have `r length(c2)` different values of the `c_2` coefficient.  For each `c_2` value, we have the `r length(x_grid)` policy function in the vector `D`, giving the harvest index number in `h_grid` optimal for the corresponding stock size in `x_grid`, and the `r length(h_grid)` by `r length(x_grid)` matrices corresponding to the realized value and penalty-free value of harvesting.  

#Extract the policy cost.  Values are expected total (net) discounted (present) value given the optimal decision under the given cost model.  That value is computed for all possible previous states.  


#We return a value matrix giving the npv for each possible $x_0$ for each possible h.  of course we don't care for each possible h, only the one that is optimal given that starting condition.  

fees <- 
lapply(policies, function(penalty){
  i_vals <- 1:length(x_grid)
  names(i_vals) <- as.character(x_grid) #prettyNum(x_grid, 3)
  lapply(i_vals, function(i){
    names(penalty) <- as.character(c2)  #prettyNum(c2, digits=4)
    lapply(penalty, function(c2_run){
      max(c2_run$V[i,])    # 
      #max(c2_run$penalty_free_V[i,])  
    })
  })
})
fees <- melt(fees)
names(fees) <- c("value", "c2", "stock_size", "model")
fees$stock_size <- as.numeric(fees$stock_size)
fees$c2 <- as.numeric(fees$c2)


## some subsetting
dt <- subset(subset(fees, stock_size > x_grid[1]), model %in% c("L1", "L2", "fixed"))
x = as.numeric(levels(as.factor(dt$stock_size))[8])
y = as.numeric(levels(as.factor(dt$stock_size))[42])

## Add npv0 as a column:
fees <- data.table(fees)
npv0 <- fees[,max(value), by=stock_size]
fees <- merge(fees, npv0, "stock_size")
setnames(fees, "V1", "npv0")

fees1 <- subset(subset(fees, stock_size == y), model %in% c("L1", "L2", "fixed"))
fees1 <- data.frame(fees1, sigma_g = sigma_g)
}
````


```{r}
low_noise <- fees_fn(0.05)
med_noise <- fees_fn(0.2)
high_noise <- fees_fn(0.5)

fees <- rbind(low_noise, med_noise, high_noise)
```





```{r}
fees$model <- as.factor(fees$model)
col.defs = c("the initial stock size, (arbitrary units)", 
             "Expected net present value", 
             "the coefficient scaling the magnitude of the adjustment costs",
             "the model",
             "the net present value of the stock size if there where no adjustment costs",
             "the environmental noise level (sigma parameter in multiplicative log-normal noise)")
unit.defs = list("tonnes", 
              "dollar/dollar", 
              "model dependent units", 
              c(L1 = "linear adjustment costs model", 
                L2 = "quadratic adjustment costs model", 
                fixed = "fixed transaction fee for every adjustment"),
              "dollar/dollar",
              "tonnes")
require(reml)
eml_write(data.set(fees, col.defs = col.defs, unit.defs = unit.defs), creator = "Carl Boettiger <cboettig@ropensci.org>")
```



```{r}

fees <- data.table(fees)
ratio <- function(npv0, value){ 
  x = 0.25
  v = (npv0-value)/npv0
  which.min(abs(v-x))
  }

fees[,ratio(npv0, value), by="model"]

a = ddply(fees, .(npv0, value), ratio)


```






```{r}
fees <- med_noise 
```

Obviously the expected net present value (ENPV) increases monotonically with starting stock size, and decreases with larger penalties (successively lower curves in the same model).  

```{r}
ggplot(subset(subset(fees, stock_size > x_grid[1]), model %in% c("L1", "L2", "fixed"))) + 
  geom_line(aes(stock_size, value, group=interaction(c2,model), color=model), alpha=.7) +
  facet_wrap(~model)

```

We need overlapping region of c2 values to be able to find apples-to-apples comparisons.  

Different starting stock sizes are just a vertical translation of the value vs penalty curve.  The exponential-like decline of value with penalty magnitude motivates our sampling of the c2 values (indicated by dots)

```{r}
ggplot(subset(dt, stock_size %in% c(x,y))) + 
  geom_line(aes(c2, value, group=interaction(stock_size, model), color=model)) + 
  facet_wrap(~stock_size) +
  geom_point(data= data.frame(c2 = c2, y=0), aes(c2,y)) # + scale_x_log10()
```



```{r}
ggplot(subset(fees, stock_size > x_grid[2] )) + 
  geom_line(aes(stock_size, value, group=interaction(c2,model), color=model), alpha=.7) +
  facet_wrap(~model)

```





Tidy up the data and plot the net present value (before the penalty has been paid) relative to that achieved when managed without a penalty.  


Find the value of `c2` that brings each penalty closest to 75% of the cost-free adjustment value:

```{r apples_plot, dependson="quadcosts"}
ggplot(subset(fees, stock_size == y), 
       aes(c2, (npv0-value)/npv0, col=model, group = interaction(model, stock_size)))  +
  geom_line()
````

Write fees out along with nusiance variables.  We will subset on a specific starting stock size since this is just a constant shift to the value curve.  







